# rl-paper-study

Reinforcement Learning paper review study

## 1st paper list

Date | Paper | Presenter | Links
:---: | :---: | :---: | :---:
5/11 | Playing Atari with Deep Reinforcement Learning, Mnih et al, 2013. | Ingyun Ahn | [[paper]](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) [[review]](./1st/200511%20-%20Playing%20Atari%20with%20Deep%20Reinforcement%20Learning%2C%20Mnih%20et%20al%2C%202013.pdf)
5/11 | Dueling Network Architectures for Deep Reinforcement Learning, Wang et al, 2015. | Jaeyoung Ahn | [[paper]](https://arxiv.org/abs/1511.06581) [[review]](./1st/200511%20-%20Dueling%20Network%20Architectures%20for%20Deep%20Reinforcement%20Learning%2C%20Wang%20et%20al%2C%202015.pdf)
5/25 | Deep Reinforcement Learning with Double Q-learning, Hasselt et al 2015. | Do-Hoon Kim | [[paper]](https://arxiv.org/abs/1509.06461) [[review]](./1st/200525%20-%20Deep%20Reinforcement%20Learning%20with%20Double%20Q-learning%2C%20Hasselt%20et%20al%202015.pdf)
5/25 | Asynchronous Methods for Deep Reinforcement Learning, Mnih et al, 2016. | Seungyoun Shin | [[paper]](https://arxiv.org/abs/1602.01783) [[review]](./1st/200525%20-%20Asynchronous%20Methods%20for%20Deep%20Reinforcement%20Learning%2C%20Mnih%20et%20al%2C%202016.pdf)
6/1 | Continuous Control With Deep Reinforcement Learning, Lillicrap et al, 2015. | Chris Ohk | [[paper]](https://arxiv.org/abs/1509.02971) [[review]](./1st/200601%20-%20Continuous%20Control%20With%20Deep%20Reinforcement%20Learning%2C%20Lillicrap%20et%20al%2C%202015.pdf)
6/1 | Mastering the game of Go with deep neural networks and tree search, D. Silver et al, Nature, 2016. | Minseok Seong | [[paper]](https://www.nature.com/articles/nature16961) [[review]](./1st/200601%20-%20Mastering%20the%20game%20of%20Go%20with%20deep%20neural%20networks%20and%20tree%20search%2C%20D.%20Silver%20et%20al%2C%20Nature%2C%202016.pdf)
6/8 | Curiosity-driven Exploration by Self-supervised Prediction, Pathak et al, 2017. | Haneul Choi | [[paper]](https://arxiv.org/abs/1705.05363) [[review]](./1st/200608%20-%20Curiosity-driven%20Exploration%20by%20Self-supervised%20Prediction%2C%20Pathak%20et%20al%2C%202017.pdf)
6/8 | Mastering the game of Go without human knowledge, D. Silver et al, Nature, 2017. | Donggu Kang | [[paper]](https://www.nature.com/articles/nature24270) [[review]](./1st/200608%20-%20Mastering%20the%20game%20of%20Go%20without%20human%20knowledge%2C%20D.%20Silver%20et%20al%2C%20Nature%2C%202017.pdf)
6/22 | Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model, J. Schrittwieser et al, 2019. | Yunhyeok Kwak | [[paper]](https://arxiv.org/pdf/1911.08265) [[review]](./1st/200622%20-%20Mastering%20Atari%2C%20Go%2C%20Chess%20and%20Shogi%20by%20Planning%20with%20a%20Learned%20Model%2C%20J.%20Schrittwieser%20et%20al%2C%202019.pdf)
6/29 | Contextual Decision Processes with low Bellman rank are PAC-Learnable, N. Jiang et al, 2017. | Hoesung Ryu | [[paper]](https://arxiv.org/abs/1610.09512) [[review]](./1st/200629%20-%20Contextual%20Decision%20Processes%20with%20low%20Bellman%20rank%20are%20PAC-Learnable%2C%20N.%20Jiang%20et%20al%2C%202017.pdf)
6/29 | Evolution Strategies as a Scalable Alternative to Reinforcement Learning, Salimans et al, 2017. | Chanhyuk Park | [[paper]](https://arxiv.org/abs/1703.03864) [[review]](./1st/200629%20-%20Evolution%20Strategies%20as%20a%20Scalable%20Alternative%20to%20Reinforcement%20Learning%2C%20Salimans%20et%20al%2C%202017.pdf)
6/29 | QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation, Kalashnikov et al, 2018. | Hyecheol (Jerry) Jang | [[paper]](https://arxiv.org/abs/1806.10293) [[review]](./1st/200629%20-%20QT-Opt%20Scalable%20Deep%20Reinforcement%20Learning%20for%20Vision-Based%20Robotic%20Manipulation%2C%20Kalashnikov%20et%20al%2C%202018.pdf)

## 2nd paper list

Date | Paper | Presenter | Links
:---: | :---: | :---: | :---:
7/27 | Deep Recurrent Q-Learning for Partially Observable MDPs, M. Hausknecht et al, 2015. | Minsuk Sung | [[paper]](https://arxiv.org/pdf/1507.06527.pdf) [[review]](./2nd/200727%20-%20Deep%20Recurrent%20Q-Learning%20for%20Partially%20Observable%20MDPs%2C%20M.%20Hausknecht%20et%20al%2C%202015.pdf)
8/3 | Hierarchical Visuomotor Control of Humanoids, J. Merel et al, 2018. | Seonghyeon Moon | [[paper]](https://arxiv.org/pdf/1811.09656.pdf) [[review]](./2nd/200803%20-%20Hierarchical%20Visuomotor%20Control%20of%20Humanoids%2C%20J.%20Merel%20et%20al%2C%202018.pdf)
8/10 | Learning Dexterous In-Hand Manipulation, M. Andrychowicz et al, 2020. | Ingyun Ahn | [[paper]](https://arxiv.org/pdf/1808.00177.pdf) [[review]](./2nd/200810%20-%20Learning%20Dexterous%20In-Hand%20Manipulation%2C%20M.%20Andrychowicz%20et%20al%2C%202020.pdf)
8/10 | Deep Reinforcement Learning with a Natural Language Action Space, J. He et al, 2015. | Jihun Kim | [[paper]](https://arxiv.org/pdf/1511.04636.pdf) [[review]](./2nd/200810%20-%20Deep%20Reinforcement%20Learning%20with%20a%20Natural%20Language%20Action%20Space%2C%20J.%20He%20et%20al%2C%202015.pdf)
8/10 | Program Guided Agent, SH. Sun et al, 2020. | Haneul Choi | [[paper]](https://openreview.net/attachment?id=BkxUvnEYDH&name=original_pdf) [[review]](./2nd/200810%20-%20Program%20Guided%20Agent%2C%20SH.%20Sun%20et%20al%2C%202020.pdf)
8/24 | Trust Region Policy Optimization, J. Schulman et al, 2015. | Chris Ohk | [[paper]](https://arxiv.org/pdf/1502.05477.pdf) [[review]](./2nd/200824%20-%20Trust%20Region%20Policy%20Optimization%2C%20Schulman%20et%20al%2C%202015.pdf)
8/31 | Proximal Policy Optimization Algorithms, J. Schulman et al, 2017. | Chris Ohk | [[paper]](https://arxiv.org/pdf/1707.06347.pdf) [[review]](./2nd/200831%20-%20Proximal%20Policy%20Optimization%20Algorithms%2C%20Schulman%20et%20al%2C%202017.pdf)
8/31 | Implementation Matters in Deep RL: A Case Study on PPO and TRPO, L. Engstrom et al, 2020. | Yunhyeok Kwak | [[paper]](https://openreview.net/attachment?id=r1etN1rtPB&name=original_pdf) [[review]](./2nd/200831%20-%20Implementation%20Matters%20in%20Deep%20RL%20A%20Case%20Study%20on%20PPO%20and%20TRPO%2C%20L.%20Engstrom%20et%20al%2C%202020.pdf)
9/7 | Generative Adversarial Imitation Learning, J. Ho et al, 2016. | Hoesung Ryu | [[paper]](https://papers.nips.cc/paper/6391-generative-adversarial-imitation-learning.pdf) [[review]](./2nd/200907%20-%20Generative%20Adversarial%20Imitation%20Learning%2C%20J.%20Ho%20et%20al%2C%202016.pdf)
9/14 | Efficient Reductions for Imitation Learning, S. Ross et al, 2010. | Hyecheol (Jerry) Jang | [[paper]](http://proceedings.mlr.press/v9/ross10a/ross10a.pdf) [[review]](./2nd/200914%20-%20Efficient%20Reductions%20for%20Imitation%20Learning%2C%20S.%20Ross%20et%20al%2C%202010.pdf)
9/14 | Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow, XB. Peng et al, 2018. | Do-Hoon Kim | [[paper]](https://arxiv.org/pdf/1810.00821.pdf) [[review]](./2nd/200914%20-%20Variational%20Discriminator%20Bottleneck%20Improving%20Imitation%20Learning%2C%20Inverse%20RL%2C%20and%20GANs%20by%20Constraining%20Information%20Flow%2C%20XB.%20Peng%20et%20al%2C%202018.pdf)
9/14 | Grandmaster Level in StarCraft II using Multi-agent Reinforcement Learning, O. Vinyals et al, 2019. | Donggu Kang | [[paper]](https://deepmind.com/research/publications/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning) [[review]](./2nd/200914%20-%20Grandmaster%20Level%20in%20StarCraft%20II%20using%20Multi-agent%20Reinforcement%20Learning%2C%20O.%20Vinyals%20et%20al%2C%202019.pdf)

## 3rd paper list

Date | Paper | Presenter | Links
:---: | :---: | :---: | :---:
11/9 | Trust Region Policy Optimization, J. Schulman et al, 2015. | Chris Ohk | [[paper]](https://arxiv.org/pdf/1502.05477.pdf) [[review]](./3rd/201109%20-%20Trust%20Region%20Policy%20Optimization%2C%20Schulman%20et%20al%2C%202015.pdf)
11/16 | Model based Reinforcement Learning for Atari, L. Kaiser et al, 2019. | Sungdong Yoo | [[paper]](https://arxiv.org/pdf/1903.00374.pdf) [[review]](./3rd/201116%20-%20Model%20based%20Reinforcement%20Learning%20for%20Atari%2C%20L.%20Kaiser%20et%20al%2C%202019.pdf)
11/16 | High-dimensional Continuous Control using Generalized Advantage Estimation, J. Schulman et al, 2015. | Junyeob Baek | [[paper]](https://arxiv.org/pdf/1506.02438.pdf) [[review]](./3rd/201116%20-%20High-dimensional%20Continuous%20Control%20using%20Generalized%20Advantage%20Estimation%2C%20J.%20Schulman%20et%20al.pdf)
11/23 | The Option-critic Architecture, PL. Bacon et al, 2017. | Seonghyeon Moon | [[paper]](https://arxiv.org/pdf/1609.05140.pdf) [[review]](./3rd/201123%20-%20The%20Option-critic%20Architecture%2C%20PL.%20Bacon%20et%20al%2C%202017.pdf)
11/23 | Rainbow: Combining Improvements in Deep Reinforcement Learning, M. Hessel et al, 2017. | Sungkwon On | [[paper]](https://arxiv.org/pdf/1710.02298.pdf) [[review]](./3rd/201123%20-%20Rainbow%20Combining%20Improvements%20in%20Deep%20Reinforcement%20Learning%2C%20M.%20Hessel%20et%20al%2C%202017.pdf)
11/30 | Prioritized Experience Replay, T. Schaul et al, 2015. | Donggu Kang | [[paper]](https://arxiv.org/pdf/1511.05952.pdf) [[review]](./3rd/201130%20-%20Prioritized%20Experience%20Replay%2C%20T.%20Schaul%20et%20al%2C%202015.pdf)
12/7 | Distributed Prioritized Experience Replay, D. Horgan et al, 2018. | Jungyeon Lee | [[paper]](https://arxiv.org/pdf/1803.00933.pdf) [[review]](./3rd/201207%20-%20Distributed%20Prioritized%20Experience%20Replay%2C%20D.%20Horgan%20et%20al%2C%202018.pdf)
12/7 | IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-learner Architectures, L. Espeholt et al, 2018. | Junhyung Kang | [[paper]](https://arxiv.org/pdf/1802.01561.pdf) [[review]](./3rd/201207%20-%20IMPALA%20Scalable%20Distributed%20Deep-RL%20with%20Importance%20Weighted%20Actor-learner%20Architectures%2C%20L.%20Espeholt%20et%20al%2C%202018.pdf)
12/14 | A Distributional Perspective on Reinforcement Learning, MG. Bellemare et al, 2017. | Wonwoo Choi | [[paper]](https://arxiv.org/pdf/1707.06887.pdf) [[review]](./3rd/201214%20-%20A%20Distributional%20Perspective%20on%20Reinforcement%20Learning%2C%20MG.%20Bellemare%20et%20al%2C%202017.pdf)
12/14 | Addressing Function Approximation Error in Actor-critic Methods, S. Fujimoto et al, 2018. | Sooyoung Lee | [[paper]](https://arxiv.org/pdf/1802.09477.pdf) [[review]](.3rd/201214%20-%20Addressing%20Function%20Approximation%20Error%20in%20Actor-critic%20Methods%2C%20S.%20Fujimoto%20et%20al%2C%202018.pdf)
12/21 | Action-gap Phenomenon in Reinforcement Learning, A. Farahmand et al, 2011. | Handong Im | [[paper]](http://papers.neurips.cc/paper/4485-action-gap-phenomenon-in-reinforcement-learning.pdf) [review]
12/21 | Soft Actor-critic: Off-policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor, T. Haarnoja et al, 2018. | Hyo Jeon | [[paper]](https://arxiv.org/pdf/1801.01290.pdf) [review]
12/28 | Second-order Optimization for Deep Reinforcement Learning using Kronecker-factored Approximation, Y. Wu et al, 2017. | Youngjin Jung | [[paper]](https://arxiv.org/pdf/1708.05144.pdf) [review]
12/28 | Proximal Policy Optimization Algorithms, J. Schulman et al, 2017. | Chris Ohk | [[paper]](https://arxiv.org/pdf/1707.06347.pdf) [review]
12/28 | Agent57: Outperforming the Atari Human Benchmark, Badia, A. P. et al, 2020. | Chris Ohk | [[paper]](https://arxiv.org/pdf/2003.13350.pdf) [review]

## Contact

You can contact me via e-mail (utilForever at gmail.com). I am always happy to answer questions or help with any issues you might have, and please be sure to share any additional work or your creations with me, I love seeing what other people are making.

## License

<img align="right" src="http://opensource.org/trademarks/opensource/OSI-Approved-License-100x137.png">

The class is licensed under the [MIT License](http://opensource.org/licenses/MIT):

Copyright (c) 2020 RL Paper Study Team

  * [Chris Ohk](http://www.github.com/utilForever)
  * [Yunhyeok Kwak](https://github.com/yun-kwak)
  * [Ingyun Ahn](https://github.com/goltong1)
  * Jaeyoung Ahn
  * [Do-Hoon Kim](https://github.com/bsstayo)
  * [Seungyoun Shin](https://github.com/SeungyounShin)
  * [Haneul Choi](https://github.com/caelum02)
  * [Minsuk Sung](https://github.com/mssung94)
  * [Donggu Kang](https://github.com/HERIUN)
  * Hoesung Ryu
  * [Chanhyuk Park](https://github.com/WithM2)
  * [Hyecheol (Jerry) Jang](https://github.com/hyecheol123)
  * [Jihun Kim](https://github.com/kgh6784)
  * Seonghyeon Moon
  * [Sungdong Yoo](https://github.com/sungdongy)
  * [Junyeob Beak](https://github.com/CUN-bjy)
  * Sungkwon On
  * [Hyo Jeon](https://github.com/hyojeon)
  * [Jungyeon Lee](https://github.com/curieuxjy)
  * [Junhyung Kang](https://github.com/JunHyungKang)
  * [Wonwoo Choi](https://github.com/deepwonwoo)
  * [Sooyoung Lee](https://github.com/soo02-ai)
  * Handong Im
  * [Youngjin Jung](https://github.com/Jung0Jin)

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
